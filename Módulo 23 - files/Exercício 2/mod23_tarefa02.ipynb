{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166f6f33-c507-4fc6-9ebd-9986eb51ea53",
   "metadata": {},
   "source": [
    "### **Módulo 23** | Combinação de modelos I | Exercício 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6a816-ff6f-403b-8622-0ca33cea960b",
   "metadata": {},
   "source": [
    "### Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01af968-8131-4047-936f-ee55315b09a4",
   "metadata": {},
   "source": [
    "**1. Monte um passo a passo para o algoritmo RF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cc51a-cdf8-44ae-b8d8-93ebce1009e1",
   "metadata": {},
   "source": [
    "O Random Forest é uma técnica avançada de ensemble em Machine Learning que aprimora e expande os princípios do Bagging por meio de uma abordagem estruturada em três etapas fundamentais:\n",
    "\n",
    "1. **Bootstrap + Seleção de Atributos**: Mantendo a essência do Bagging, o Random Forest começa com a geração de amostras aleatórias com reposição do conjunto de dados de treinamento original. A novidade reside na seleção de um subconjunto aleatório de atributos para cada amostra. Essa seleção visa aumentar a diversidade entre as árvores de decisão geradas, reduzindo a correlação entre elas. Na prática, para problemas de classificação, recomenda-se escolher a raiz quadrada do número total de atributos; para problemas de regressão, utiliza-se comumente um terço do total de atributos.\n",
    "\n",
    "2. **Modelagem com Árvores de Decisão**: Cada amostra bootstrap, acompanhada de seu respectivo conjunto de atributos selecionados aleatoriamente, é usada para treinar uma árvore de decisão de forma independente. Esta etapa capitaliza na capacidade das árvores de decisão de lidar com interações complexas entre atributos, permitindo que cada árvore aprenda de uma perspectiva única dentro do espaço de atributos.\n",
    "\n",
    "3. **Agregação**: A etapa final consolida os resultados de todas as árvores de decisão individuais em uma única previsão coletiva. Para tarefas de classificação, isso é realizado por meio de uma votação majoritária, onde a classe mais frequentemente prevista pelas árvores é escolhida como a resposta definitiva. Em contextos de regressão, a agregação é feita calculando a média das previsões de todas as árvores, produzindo uma estimativa final equilibrada.\n",
    "\n",
    "O método Random Forest se destaca por sua robustez e capacidade de minimizar o overfitting, graças à diversidade introduzida pela seleção aleatória de atributos e pela construção de múltiplas árvores de decisão. Essas características tornam o Random Forest uma ferramenta valiosa e amplamente aplicada para resolver uma vasta gama de problemas preditivos em Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805122b5-5f98-4013-bac2-d36ed341325c",
   "metadata": {},
   "source": [
    "**2. Explique com suas palavras o Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7b205-08c5-45a8-a29e-45e1c9baf2b5",
   "metadata": {},
   "source": [
    "O Random Forest é uma técnica sofisticada de Machine Learning que aprimora os fundamentos do Bootstrap Aggregating, ou Bagging, introduzindo eficiências significativas tanto em desempenho quanto em precisão. No cerne do Random Forest está a combinação estratégica de múltiplos modelos, cada um treinado em distintas variações do conjunto de dados original. Estas variações são geradas por meio de amostras bootstrap, que são subconjuntos aleatórios do conjunto de dados original, permitindo repetições mas preservando o número original de observações.\n",
    "\n",
    "A inovação do Random Forest se manifesta na sua abordagem seletiva de variáveis para cada modelo de árvore de decisão envolvido. Ao limitar cada árvore a um subconjunto aleatório de variáveis, o Random Forest busca não apenas diversificar as perspectivas de aprendizado de cada modelo, mas também diminuir a correlação entre as árvores, reduzindo substancialmente a variância dos resultados finais e mitigando o risco de overfitting.\n",
    "\n",
    "Para sintetizar uma previsão final a partir do conjunto de modelos, o Random Forest emprega um método de agregação similar ao Bagging: utiliza-se a média das previsões para problemas de regressão e a votação majoritária para problemas de classificação. Essa estratégia de consenso não apenas capitaliza na sabedoria coletiva dos modelos individuais, mas também assegura que o resultado final seja mais equilibrado e resistente a anomalias específicas de qualquer modelo singular.\n",
    "\n",
    "Em suma, o Random Forest representa uma evolução natural e poderosa do Bagging, destacando-se por sua capacidade de entregar previsões altamente precisas e confiáveis, ao mesmo tempo em que mantém um controle rigoroso sobre o overfitting, tornando-o uma ferramenta indispensável no arsenal de técnicas de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54f311-a631-4d9c-a2e0-92064484db82",
   "metadata": {},
   "source": [
    "**3. Qual a diferença entre Bagging e Random Forest?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd86441-282d-434f-8a6c-c84f668a7a31",
   "metadata": {},
   "source": [
    "A principal distinção entre Bagging e Random Forest reside na especificidade da aplicação e na eficácia na minimização da variância dos resultados preditivos. O Bagging representa uma estratégia ampla de combinação de modelos em Machine Learning, fundamentada no princípio da amostragem aleatória com reposição para criar múltiplos subconjuntos do conjunto de dados original. Esta técnica promove a diversidade entre os modelos treinados, visando reduzir a variância sem incrementar significativamente o viés.\n",
    "\n",
    "Por outro lado, o Random Forest é uma evolução especializada do Bagging, focada exclusivamente no uso de árvores de decisão. Nesta abordagem, além da amostragem aleatória de observações (amostras bootstrap), uma seleção aleatória de variáveis é realizada para cada árvore treinada. Essa dupla aleatoriedade - tanto nas amostras quanto nas variáveis - confere às árvores de decisão uma independência maior entre si, diminuindo a correlação e, consequentemente, reduzindo ainda mais a variância dos resultados combinados.\n",
    "\n",
    "A redução na correlação entre as árvores é um aspecto chave que distingue e eleva o Random Forest sobre o Bagging genérico. Ao limitar o número de variáveis disponíveis para cada árvore, o Random Forest assegura que diferentes árvores se concentrem em diferentes aspectos dos dados, aumentando a robustez e a acurácia do modelo agregado. Isso resulta em previsões finais que são tipicamente mais precisas e confiáveis do que aquelas obtidas apenas pelo Bagging.\n",
    "\n",
    "Portanto, o Random Forest pode ser considerado uma extensão refinada do Bagging, oferecendo melhorias significativas em desempenho por meio da redução de variância e da promoção de modelos menos correlacionados. Essa característica torna o Random Forest particularmente eficaz em enfrentar o desafio do overfitting, ao mesmo tempo em que mantém a capacidade de capturar complexidades nos dados, o que justifica sua preferência em muitas aplicações práticas de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478db89-7ae8-4216-a600-9f5c97f3cbf0",
   "metadata": {},
   "source": [
    "**4.** (Opcional) Implementar em python o Random Forest\n",
    "> - Bootstrap\n",
    "> - Feature selection\n",
    "> - Modelagem com Decision trees\n",
    "> - Agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a698d4e7-cef9-411a-9227-f6a0ce9e5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import das bibliotecas:\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets        import load_iris\n",
    "from sklearn.datasets        import load_diabetes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.metrics         import accuracy_score\n",
    "\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.metrics         import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9eebb7d-9eda-475d-87f6-a8565dd80612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Accuracy score: 0.96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_test  y_pred\n",
       "0        0       0\n",
       "1        2       2\n",
       "2        1       1\n",
       "3        2       2\n",
       "4        1       1\n",
       "5        2       2\n",
       "6        2       1\n",
       "7        2       2\n",
       "8        1       1\n",
       "9        0       0\n",
       "10       2       2\n",
       "11       2       1\n",
       "12       1       1\n",
       "13       0       0\n",
       "14       0       0\n",
       "15       0       0\n",
       "16       1       1\n",
       "17       1       1\n",
       "18       0       0\n",
       "19       2       2\n",
       "20       2       2\n",
       "21       0       0\n",
       "22       2       2\n",
       "23       2       2\n",
       "24       1       1\n",
       "25       0       0\n",
       "26       0       0\n",
       "27       2       2\n",
       "28       1       1\n",
       "29       0       0\n",
       "30       0       0\n",
       "31       0       0\n",
       "32       0       0\n",
       "33       2       2\n",
       "34       2       2\n",
       "35       1       1\n",
       "36       1       1\n",
       "37       0       0\n",
       "38       2       2\n",
       "39       1       1\n",
       "40       0       0\n",
       "41       2       2\n",
       "42       2       2\n",
       "43       0       0\n",
       "44       2       2\n",
       "45       2       2\n",
       "46       0       0\n",
       "47       1       1\n",
       "48       1       1\n",
       "49       1       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo da técnica Random Forest para problemas de classificação:\n",
    "\n",
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "df = pd.DataFrame(X, columns=load_iris().feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "def rf_classifier(df:pd.DataFrame, \n",
    "                  num_bootstrap_samples:int=3,  # Parâmetro da função que define a quantidade de amostragens para treinamento\n",
    "                  test_size:float=0.25\n",
    "                 ) -> pd.DataFrame:\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=test_size)\n",
    "    \n",
    "    X_test = df_test.drop(['target'], axis=1)\n",
    "    y_test = df_test['target'].rename('y_test')\n",
    "    \n",
    "    # Dicionário para os resultados das predições de cada modelo\n",
    "    y_pred_bagging = {}\n",
    "\n",
    "    for i in range(num_bootstrap_samples):\n",
    "        # Bootstrap\n",
    "        df_train = df_train.sample(n=len(df_train), \n",
    "                                   replace=True)  # Amostragem COM reposição\n",
    "\n",
    "        X_train = df_train.drop(['target'], axis=1)\n",
    "        # Feature selection\n",
    "        X_train = X_train.sample(n=round(np.sqrt(X_train.shape[1])),  # Cálculo da raiz quadrada da quantidade de variáveis\n",
    "                                 axis=1)\n",
    "        \n",
    "        y_train = df_train['target']\n",
    "        \n",
    "        # Modelagem (base learners)\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Adicionando os resultados do modelo ao dicionário para agregação das predições\n",
    "        y_pred_bagging.update({i:model.predict(X_test[X_train.columns])})\n",
    "    \n",
    "    # Aggregating\n",
    "    y_pred = (pd.DataFrame(y_pred_bagging)\n",
    "                .mode(axis=1)  # Agregando o valor com maior número de aparições nas predições dos modelos\n",
    "                .rename(columns={0:'y_pred'}))\n",
    " \n",
    "    # Resultados\n",
    "    print(model)\n",
    "    print('Accuracy score:', accuracy_score(y_true=y_test, \n",
    "                                            y_pred=y_pred['y_pred']\n",
    "                                           ))\n",
    "\n",
    "    return pd.concat(objs=[y_test.reset_index(drop=True), \n",
    "                           y_pred['y_pred'].astype(int)], \n",
    "                     axis=1)\n",
    "\n",
    "rf_classifier(num_bootstrap_samples=10, df=df, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44692d5-b1a0-4351-af50-6941709045c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor()\n",
      "Mean squared error: 4199.046575342466\n",
      "Coefficient of determination: 0.271689234225421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155.0</td>\n",
       "      <td>121.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>155.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.0</td>\n",
       "      <td>187.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154.0</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>148.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>198.0</td>\n",
       "      <td>123.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>199.0</td>\n",
       "      <td>133.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>270.0</td>\n",
       "      <td>190.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>91.0</td>\n",
       "      <td>163.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test  y_pred\n",
       "0     155.0   121.3\n",
       "1      72.0   155.2\n",
       "2     197.0   187.5\n",
       "3     154.0   141.5\n",
       "4     277.0   217.0\n",
       "..      ...     ...\n",
       "141   148.0   153.0\n",
       "142   198.0   123.6\n",
       "143   199.0   133.3\n",
       "144   270.0   190.4\n",
       "145    91.0   163.3\n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo da técnica Random Forest para problemas de regressão:\n",
    "\n",
    "X = load_diabetes().data\n",
    "y = load_diabetes().target\n",
    "\n",
    "df = pd.DataFrame(X, columns=load_diabetes().feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "def rf_regressor(df:pd.DataFrame, \n",
    "                 num_bootstrap_samples:int=3,  # Parâmetro da função que define a quantidade de amostragens para treinamento\n",
    "                 test_size:float=0.25\n",
    "                ) -> pd.DataFrame:\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=test_size)\n",
    "    \n",
    "    X_test = df_test.drop(['target'], axis=1)\n",
    "    y_test = df_test['target'].rename('y_test')\n",
    "    \n",
    "    # Dicionário para os resultados das predições de cada modelo\n",
    "    y_pred_bagging = {}\n",
    "\n",
    "    for i in range(num_bootstrap_samples):\n",
    "        # Bootstrap\n",
    "        df_train = df_train.sample(n=len(df_train), \n",
    "                                   replace=True)  # Amostragem COM reposição\n",
    "\n",
    "        X_train = df_train.drop(['target'], axis=1)\n",
    "        # Feature selection\n",
    "        X_train = X_train.sample(n=round(X_train.shape[1]/3),  # Cálculo da quantidade de variáveis dividida por 3\n",
    "                                 axis=1)\n",
    "        \n",
    "        y_train = df_train['target']\n",
    "\n",
    "        # Modelagem (base learners)\n",
    "        model = DecisionTreeRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Adicionando os resultados do modelo ao dicionário para agregação das predições\n",
    "        y_pred_bagging.update({i:model.predict(X_test[X_train.columns])})\n",
    "\n",
    "    # Aggregating\n",
    "    y_pred = (pd.DataFrame(y_pred_bagging)\n",
    "                .mean(axis=1)  # Agregando as predições dos modelos baseando n a média dos resultados\n",
    "                .rename('y_pred'))\n",
    " \n",
    "    # Resultados\n",
    "    print(model)\n",
    "    print('Mean squared error:', mean_squared_error(y_true=y_test, \n",
    "                                                   y_pred=y_pred))\n",
    "    print('Coefficient of determination:', r2_score(y_true=y_test, \n",
    "                                                    y_pred=y_pred))\n",
    "    \n",
    "    return pd.concat(objs=[y_test.reset_index(drop=True), \n",
    "                           y_pred], \n",
    "                     axis=1)\n",
    "    \n",
    "rf_regressor(num_bootstrap_samples=10, df=df, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afea5b5-6bed-4c12-adf1-988ce95e0db1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
