{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01fe748-314a-4fa9-bbc8-b58cd625898d",
   "metadata": {},
   "source": [
    "### **Módulo 24** | Combinação de modelos II | Exercício 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87421f-2533-4f0f-9df1-61d76430e0bc",
   "metadata": {},
   "source": [
    "### Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8c515-70bd-4e3d-a35e-267fc6101275",
   "metadata": {},
   "source": [
    "**1.** Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b6575-ec87-489c-81f1-b49a29cbba70",
   "metadata": {},
   "source": [
    "O AdaBoost (Adaptive Boosting) e o Gradient Boosting Machine (GBM) são ambos algoritmos de boosting populares em aprendizado de máquina, que criam um modelo forte a partir de uma série de modelos fracos. Apesar de compartilharem algumas semelhanças conceituais, eles têm diferenças significativas em sua implementação e funcionamento. Aqui estão cinco diferenças principais entre AdaBoost e GBM:\n",
    "\n",
    "1. Abordagem de Ponderação de Erros:\n",
    "AdaBoost: Ajusta os pesos dos exemplos no conjunto de dados com base nos erros do modelo anterior. Os exemplos mal classificados recebem mais peso, fazendo com que o modelo subsequente foque mais neles.\n",
    "GBM: Utiliza o gradiente do erro da função de perda para ajustar os modelos subsequentes. A ideia é minimizar a função de perda ao ajustar os modelos para prever o gradiente de erro do modelo anterior.\n",
    "\n",
    "\n",
    "2. Função de Perda:\n",
    "AdaBoost: Geralmente usa uma função de perda exponencial, embora seja restrito a problemas de classificação e regressão binária em suas implementações mais comuns.\n",
    "GBM: É mais flexível em relação à função de perda e pode ser usado com diferentes funções de perda, tornando-o aplicável a uma gama mais ampla de problemas, incluindo regressão e classificação multiclasse.\n",
    "\n",
    "\n",
    "3. Tratamento de Dados Ruidosos e Outliers:\n",
    "AdaBoost: É bastante sensível a dados ruidosos e outliers porque dá mais peso aos exemplos difíceis de classificar, o que pode levar a um desempenho pior se esses dados não forem representativos.\n",
    "GBM: Embora também possa ser afetado por dados ruidosos e outliers, sua flexibilidade na escolha da função de perda permite uma certa adaptação a esses desafios, potencialmente oferecendo um melhor desempenho em conjuntos de dados com essas características.\n",
    "\n",
    "\n",
    "4. Velocidade e Eficiência Computacional:\n",
    "AdaBoost: Pode ser mais rápido de treinar do que GBM porque simplesmente ajusta os pesos dos exemplos ao invés de calcular gradientes, o que pode ser computacionalmente menos intensivo.\n",
    "GBM: Pode ser mais demorado e exigir mais recursos computacionais devido ao cálculo dos gradientes e à necessidade de otimizar a função de perda a cada iteração.\n",
    "\n",
    "\n",
    "5. Complexidade dos Modelos Base:\n",
    "AdaBoost: Tradicionalmente, utiliza aprendizes muito simples (como árvores de decisão de um nível, também conhecidas como 'stumps') como modelos base.\n",
    "GBM: Permite o uso de modelos base mais complexos e é comum utilizar árvores de decisão com profundidade maior do que um, proporcionando uma modelagem mais detalhada dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6d6ef-fc2c-4a3b-af12-84f47a045206",
   "metadata": {},
   "source": [
    "**2.** Acesse o link [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ae2ef-1da6-4003-88b2-07493906dc17",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.1. Classification](https://scikit-learn.org/stable/modules/ensemble.html#classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, \n",
    "                                 learning_rate=1.0, \n",
    "                                 max_depth=1, \n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b0c1c-4407-4b0c-b4c9-e6fb8efd7bf2",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.2. Regression](https://scikit-learn.org/stable/modules/ensemble.html#regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39199baf-62e1-493b-a692-0fb21529054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=1, \n",
    "                                random_state=0, \n",
    "                                loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875633aa-d96e-460b-8487-a6743ef61926",
   "metadata": {},
   "source": [
    "**3.** Cite 5 hiperparâmetros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9",
   "metadata": {},
   "source": [
    "No Gradient Boosting Machine (GBM), cinco hiperparâmetros desempenham papéis cruciais na definição da estrutura e na eficácia do modelo:\n",
    "\n",
    "1. **n_estimators:** Esse hiperparâmetro fundamental determina o número de árvores sequenciais a serem construídas no modelo. Aumentar esse valor pode melhorar a precisão do modelo, mas também eleva o tempo necessário para o treinamento e a complexidade do modelo, podendo levar a um risco maior de overfitting.\n",
    "\n",
    "\n",
    "2. **learning_rate:** Representa a contribuição de cada árvore para a previsão final e é um fator crítico que influencia diretamente a performance do modelo. Valores menores necessitam de mais árvores (um `n_estimators` maior) para manter uma performance robusta, estabelecendo um equilíbrio delicado entre a taxa de aprendizado e o número de estimadores.\n",
    "\n",
    "\n",
    "3. **max_depth:** Esse parâmetro define a profundidade máxima permitida para cada árvore, controlando assim a complexidade das árvores geradas. Profundidades maiores possibilitam que o modelo capture relações mais complexas, mas aumentam o risco de overfitting ao ajustar demais aos dados de treinamento.\n",
    "\n",
    "\n",
    "4. **max_features:** Determina o número máximo de recursos considerados para a divisão de cada nó das árvores, afetando a diversidade das árvores geradas. Ajustar esse parâmetro pode ajudar a prevenir overfitting, garantindo que cada árvore tenha uma perspectiva única dos dados ao limitar o número de recursos avaliados.\n",
    "\n",
    "\n",
    "5. **warm_start:** Essa opção permite a incrementação do modelo existente com novas árvores sem iniciar o treinamento do zero, mantendo as árvores já treinadas e adicionando novas para aprimorar o modelo. É particularmente útil para ajustes finos ou quando mais dados se tornam disponíveis.\n",
    "\n",
    "A otimização desses hiperparâmetros é essencial para maximizar a eficácia do GBM, adaptando o modelo às especificidades do conjunto de dados e às exigências do problema. Experimentar com diferentes configurações desses parâmetros pode levar a melhorias significativas no desempenho do modelo.\n",
    "\n",
    ">> Exemplo:\n",
    ">>```python\n",
    ">>>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    ">>>>> _ = est.fit(X_train, y_train)  # fit additional 100 trees to est\n",
    ">>>>> mean_squared_error(y_test, est.predict(X_test))\n",
    ">>3.84...\n",
    ">>```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d6267-19e7-47d2-b795-a68bcd361515",
   "metadata": {},
   "source": [
    "**4.** (**Opcional**) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac016b-f9cb-4e7c-a2b4-12b164f21e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d473856d-bb0c-495e-a17d-2632eb805035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 783 ms, total: 1min 25s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>3.667692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>21.909960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9</td>\n",
       "      <td>22.400111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>22.472923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>22.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>24.252178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate  max_depth  mean_squared_error\n",
       "0           1000           0.03          1            3.625209\n",
       "1           1000           0.06          1            3.667692\n",
       "2          10000           0.10          3            3.733205\n",
       "3           1000           0.10          3            3.733272\n",
       "4            100           0.10          3            3.774987\n",
       "..           ...            ...        ...                 ...\n",
       "59            10           0.03          1           21.909960\n",
       "60            10           0.01          9           22.400111\n",
       "61            10           0.01          6           22.472923\n",
       "62            10           0.01          3           22.902975\n",
       "63            10           0.01          1           24.252178\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimators     = [10, 100, 1000, 10000]\n",
    "learning_rates = [0.01, 0.03, 0.06, 0.1]\n",
    "max_depths     = [1, 3, 6, 9]\n",
    "\n",
    "grid_search = []\n",
    "\n",
    "for n in estimators:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depths:\n",
    "            est = GradientBoostingRegressor(n_estimators=n, \n",
    "                                            learning_rate=rate, \n",
    "                                            max_depth=depth, \n",
    "                                            random_state=0, \n",
    "                                            loss='squared_error').fit(X_train, y_train)\n",
    "            grid_search.append([n, rate, depth, mean_squared_error(y_test, est.predict(X_test))])\n",
    "            \n",
    "(pd.DataFrame(data=grid_search, \n",
    "              columns=['n_estimators', 'learning_rate', 'max_depth', 'mean_squared_error'])\n",
    "   .sort_values(by='mean_squared_error', \n",
    "                ascending=True, \n",
    "                ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd77068-5da9-48bf-b7b7-c88ff2978b5e",
   "metadata": {},
   "source": [
    "**5.** Acessando o artigo do Jerome Friedman ([Stochastic](https://jerryfriedman.su.domains/ftp/stobst.pdf)) e pensando no nome dado ao **Stochastic GBM**, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a",
   "metadata": {},
   "source": [
    "Refletindo sobre o artigo de Jerome Friedman a respeito do Gradient Boosting estocástico e considerando o termo \"Stochastic GBM\", podemos identificar a principal distinção entre este e o GBM tradicional. O Stochastic GBM se destaca por sua abordagem que integra elementos de aleatoriedade no processo de treinamento, uma característica inspirada na teoria probabilística.\n",
    "\n",
    "O Stochastic GBM representa uma evolução dos métodos de Gradient Boosting, incorporando princípios do Bootstrap Aggregating (Bagging), o que o caracteriza como uma fusão inovadora das estratégias de Bagging e Boosting. Essencialmente, em cada etapa do treinamento, ele seleciona um subconjunto aleatório dos dados, sem reposição, geralmente usando cerca de metade do conjunto total de treinamento. Esta seleção aleatória de dados para treinar os classificadores base em cada iteração amplia a precisão do método de Gradient Boosting e confere ao modelo uma robustez superior em comparação com o GBM convencional.\n",
    "\n",
    "A injeção de aleatoriedade na seleção de amostras para cada iteração contribui significativamente para mitigar o risco de overfitting. Ao fazer isso, o Stochastic GBM promove uma melhor capacidade de generalização, adaptando-se de forma mais eficiente a dados não vistos. Portanto, essa principal diferença - a introdução de aleatoriedade na escolha das amostras - não apenas aprimora a performance do modelo mas também o torna mais resiliente contra a sobreajuste dos dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4448a-5921-4d47-84af-b29a7f008175",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
