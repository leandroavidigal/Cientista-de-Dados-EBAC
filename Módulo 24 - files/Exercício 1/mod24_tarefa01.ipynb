{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d9d7e5-dd35-4222-9f5b-2ad51ce3291f",
   "metadata": {},
   "source": [
    "## **Módulo 24** | Combinação de modelos II | Exercício 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc89696-b7e1-4be3-b493-c6b863fa337f",
   "metadata": {},
   "source": [
    "## Tarefa 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05220745-5553-46bf-9721-3e16cccd3285",
   "metadata": {},
   "source": [
    "**1.** Cite 5 diferenças entre o Random Forest e o AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db295d28-729f-4b9f-943b-bc31ccb47ce2",
   "metadata": {},
   "source": [
    "O RandomForest e o AdaBoost são ambos algoritmos de aprendizado de máquina dentro da categoria de métodos de ensemble, mas possuem diferenças fundamentais em sua concepção e funcionamento. Aqui estão cinco diferenças principais entre eles:\n",
    "\n",
    "#### 1. **Método de Construção dos Modelos:**\n",
    "- **RandomForest:** Constrói múltiplas árvores de decisão independentes durante o treinamento e utiliza a média (para regressão) ou a moda (para classificação) das previsões de todas as árvores.\n",
    "- **AdaBoost (Adaptive Boosting):** Constrói sequencialmente uma série de modelos fracos (tipicamente árvores de decisão de um único nível, também conhecidas como 'stumps'), onde cada modelo subsequente tenta corrigir os erros do modelo anterior.\n",
    "\n",
    "#### 2. **Foco na Redução de Erro:**\n",
    "- **RandomForest:** Tenta reduzir o overfitting e melhorar a robustez através da agregação das previsões de várias árvores de decisão.\n",
    "- **AdaBoost:** Foca em aumentar o peso dos exemplos que foram previstos incorretamente pelo modelo anterior, tornando o modelo subsequente mais focado em corrigir esses erros.\n",
    "\n",
    "#### 3. **Complexidade dos Modelos Componentes:**\n",
    "- **RandomForest:** Utiliza árvores de decisão completas como modelos base, que podem se aprofundar até que cada folha seja pura ou contenha menos que um mínimo de amostras definido pelo usuário.\n",
    "- **AdaBoost:** Frequentemente usa modelos muito simples (como árvores de decisão de um único nível) como os aprendizes base, embora possa ser usado com qualquer algoritmo de aprendizado de máquina.\n",
    "\n",
    "#### 4. **Ponderação das Previsões:**\n",
    "- **RandomForest:** Cada árvore no ensemble tem igual peso na votação final da previsão.\n",
    "- **AdaBoost:** Atribui pesos aos modelos baseados em sua precisão. Modelos que performam melhor recebem mais peso, fazendo com que tenham mais influência na previsão final.\n",
    "\n",
    "#### 5. **Sensibilidade a Dados Ruidosos e Outliers:**\n",
    "- **RandomForest:** É relativamente robusto a outliers e dados ruidosos devido ao processo de bootstrapping (amostragem com reposição) e ao mecanismo de votação das árvores.\n",
    "- **AdaBoost:** Pode ser sensível a dados ruidosos e outliers, pois os modelos subsequentes focam nos casos mais difíceis, que podem incluir outliers, potencialmente levando a um ajuste excessivo a esses pontos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89239d72-96bc-47e4-b991-b1e709be253c",
   "metadata": {},
   "source": [
    "**2.** Acesse o link [Scikit-learn – adaboost](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a1819c-32c7-4923-a726-316782116e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets        import load_iris\n",
    "from sklearn.ensemble        import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0424eee9-4d7c-4b5d-a49e-db5a1aba574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y   = load_iris(return_X_y=True)\n",
    "\n",
    "clf    = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(estimator=clf, \n",
    "                         X=X, \n",
    "                         y=y, \n",
    "                         cv=5)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49e8a9-17c5-4a26-8acc-c73363ef763e",
   "metadata": {},
   "source": [
    "**3.** Cite 5 Hiperparâmetros importantes no AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1efa6a0-234a-4c6e-a673-320350573444",
   "metadata": {},
   "source": [
    "1. **Estimator (estimador):** Este é o classificador base utilizado para construir os aprendizes fracos, frequentemente denominados 'Stumps'. O estimador padrão é uma árvore de decisão `DecisionTreeClassifier` configurada com `max_depth=1`. Este hiperparâmetro é a evolução do antigo `base_estimator`, ajustando-se ao aprendiz base escolhido para o processo de boosting\n",
    "\n",
    "\n",
    "2. **n_estimators (número de estimadores):** Define o número máximo de aprendizes fracos a serem gerados durante o treinamento. Embora o processo possa parar antes se a solução ideal for alcançada, um número maior de estimadores aumenta o tempo de treinamento. Este valor determina a quantidade de 'Stumps' criados.\n",
    "\n",
    "\n",
    "3. **learning_rate (taxa de aprendizado):** Especifica o peso atribuído a cada classificador em cada iteração de boosting. Ajustar a taxa de aprendizado pode alterar significativamente a contribuição de cada aprendiz fraco no resultado final, podendo existir um equilíbrio entre este parâmetro e o `n_estimators`.\n",
    "\n",
    "\n",
    "4. **algorithm (algoritmo) no AdaBoostClassifier:** Seleciona o algoritmo usado para ajustar os pesos dos dados durante o treinamento. As opções incluem SAMME (Discrete AdaBoost) e SAMME.R (Real AdaBoost), sendo o SAMME.R geralmente preferido por oferecer melhor desempenho.\n",
    "\n",
    "\n",
    "5. **loss (perda) no AdaBoostRegressor:** Para problemas de regressão, este hiperparâmetro define a função de perda usada na atualização dos pesos após cada iteração de boosting. As opções são 'linear', 'square' e 'exponential', com 'linear' sendo o valor padrão.\n",
    "\n",
    "Ajustar esses hiperparâmetros de acordo com as características específicas do conjunto de dados pode melhorar significativamente a performance do modelo AdaBoost, otimizando tanto a precisão das previsões quanto a eficiência do treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22e83d-a610-42d5-b995-a1d238f5c1d5",
   "metadata": {},
   "source": [
    "**4.** (**Opcional**) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo (load_iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417b7e95-c484-40cf-bc26-0fdd88e5d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f905aa-f7cb-460f-8522-e87a602e421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 582 ms, total: 45.9 s\n",
      "Wall time: 46.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>501</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1001</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  mean_score\n",
       "0              1    0.666667\n",
       "1            101    0.953333\n",
       "2            201    0.946667\n",
       "3            301    0.946667\n",
       "4            401    0.946667\n",
       "5            501    0.946667\n",
       "6            601    0.946667\n",
       "7            701    0.946667\n",
       "8            801    0.946667\n",
       "9            901    0.946667\n",
       "10          1001    0.946667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimators = list(range(1, 1002, 100))\n",
    "\n",
    "n_estimators = []\n",
    "mean_scores  = []\n",
    "\n",
    "for n in estimators:\n",
    "    clf    = AdaBoostClassifier(n_estimators=n)\n",
    "    scores = cross_val_score(estimator=clf, \n",
    "                             X=X, \n",
    "                             y=y, \n",
    "                             cv=5)\n",
    "    n_estimators.append(n)\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "pd.DataFrame(data=list(zip(n_estimators, mean_scores)), \n",
    "             columns=['n_estimators', 'mean_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921159d5-086f-44d0-a8dc-1f222c0e7d9e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
